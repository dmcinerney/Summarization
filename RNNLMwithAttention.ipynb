{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from PICOHelper import get_pico_datasets\n",
    "from NewsroomHelper import get_newsroom_datasets\n",
    "from SummarizationModelStructures import GeneratorModel\n",
    "from utils import DataLoader, get_index_words\n",
    "from pytorch_helper import ModelManipulator, plot_learning_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-2\n",
    "# INITIAL_ACCUMULATOR_VALUE = 0.1\n",
    "GAMMA = 1\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11029 3676 3678\n",
      "retrieving word2vec model from file\n"
     ]
    }
   ],
   "source": [
    "# pico_dataset_train, pico_dataset_dev, pico_dataset_test = get_pico_datasets()\n",
    "newsroom_dataset_train, newsroom_dataset_dev, newsroom_dataset_test = get_newsroom_datasets()\n",
    "word_vectors = newsroom_dataset_train.word_vectors\n",
    "start_index = newsroom_dataset_train.word_indices['<start>']\n",
    "end_index = newsroom_dataset_train.word_indices['<end>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_encoder.num_hidden + summary_decoder.num_hidden\n",
    "def loss(loss):\n",
    "    return loss\n",
    "\n",
    "def error(loss):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model = GeneratorModel(word_vectors, start_index, end_index, num_hidden1=None, num_hidden2=None, with_coverage=False, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, train_loss: 216.238525, train_error: None\n",
      "epoch: 0, batch: 10, train_loss: 158.997604, train_error: None\n",
      "epoch: 0, batch: 20, train_loss: 182.418015, train_error: None\n",
      "epoch: 0, batch: 30, train_loss: 208.012192, train_error: None\n",
      "epoch: 0, batch: 40, train_loss: 143.501373, train_error: None\n",
      "epoch: 0, batch: 50, train_loss: 156.999603, train_error: None\n",
      "epoch: 0, batch: 60, train_loss: 86.735733, train_error: None\n",
      "epoch: 0, batch: 70, train_loss: 185.347977, train_error: None\n",
      "epoch: 0, batch: 80, train_loss: 113.356445, train_error: None\n",
      "epoch: 0, batch: 90, train_loss: 92.096771, train_error: None\n",
      "epoch: 0, batch: 100, train_loss: 102.141060, train_error: None\n",
      "epoch: 0, batch: 110, train_loss: 96.801842, train_error: None\n",
      "epoch: 0, batch: 120, train_loss: 82.156792, train_error: None\n",
      "epoch: 0, batch: 130, train_loss: 103.128029, train_error: None\n",
      "epoch: 0, batch: 140, train_loss: 100.096527, train_error: None\n",
      "epoch: 0, batch: 150, train_loss: 103.898499, train_error: None\n",
      "epoch: 0, batch: 160, train_loss: 121.303215, train_error: None\n",
      "epoch: 0, batch: 170, train_loss: 106.183662, train_error: None\n",
      "epoch: 1, batch: 8, train_loss: 82.246117, train_error: None\n",
      "epoch: 1, batch: 18, train_loss: 106.600166, train_error: None\n",
      "epoch: 1, batch: 28, train_loss: 136.284775, train_error: None\n",
      "epoch: 1, batch: 38, train_loss: 140.619186, train_error: None\n",
      "epoch: 1, batch: 48, train_loss: 91.912804, train_error: None\n",
      "epoch: 1, batch: 58, train_loss: 67.969780, train_error: None\n",
      "epoch: 1, batch: 68, train_loss: 98.671272, train_error: None\n",
      "epoch: 1, batch: 78, train_loss: 80.682899, train_error: None\n",
      "epoch: 1, batch: 88, train_loss: 80.745567, train_error: None\n",
      "epoch: 1, batch: 98, train_loss: 86.458878, train_error: None\n",
      "epoch: 1, batch: 108, train_loss: 88.742996, train_error: None\n",
      "epoch: 1, batch: 118, train_loss: 89.377861, train_error: None\n",
      "epoch: 1, batch: 128, train_loss: 78.183334, train_error: None\n",
      "epoch: 1, batch: 138, train_loss: 70.737236, train_error: None\n",
      "epoch: 1, batch: 148, train_loss: 77.391319, train_error: None\n",
      "epoch: 1, batch: 158, train_loss: 100.079880, train_error: None\n",
      "epoch: 1, batch: 168, train_loss: 113.136047, train_error: None\n",
      "epoch: 2, batch: 6, train_loss: 86.702850, train_error: None\n",
      "epoch: 2, batch: 16, train_loss: 86.366890, train_error: None\n",
      "epoch: 2, batch: 26, train_loss: 111.018227, train_error: None\n",
      "epoch: 2, batch: 36, train_loss: 152.942642, train_error: None\n",
      "epoch: 2, batch: 46, train_loss: 102.276123, train_error: None\n",
      "epoch: 2, batch: 56, train_loss: 95.809303, train_error: None\n",
      "epoch: 2, batch: 66, train_loss: 81.391922, train_error: None\n",
      "epoch: 2, batch: 76, train_loss: 149.336716, train_error: None\n",
      "epoch: 2, batch: 86, train_loss: 94.975082, train_error: None\n",
      "epoch: 2, batch: 96, train_loss: 111.850967, train_error: None\n",
      "epoch: 2, batch: 106, train_loss: 74.910934, train_error: None\n",
      "epoch: 2, batch: 116, train_loss: 87.015366, train_error: None\n",
      "epoch: 2, batch: 126, train_loss: 76.413643, train_error: None\n",
      "epoch: 2, batch: 136, train_loss: 98.679993, train_error: None\n",
      "epoch: 2, batch: 146, train_loss: 78.713867, train_error: None\n",
      "epoch: 2, batch: 156, train_loss: 82.178741, train_error: None\n",
      "epoch: 2, batch: 166, train_loss: 75.649117, train_error: None\n",
      "epoch: 3, batch: 4, train_loss: 88.343750, train_error: None\n",
      "epoch: 3, batch: 14, train_loss: 88.863068, train_error: None\n",
      "epoch: 3, batch: 24, train_loss: 74.667877, train_error: None\n",
      "epoch: 3, batch: 34, train_loss: 102.611954, train_error: None\n",
      "epoch: 3, batch: 44, train_loss: 79.547798, train_error: None\n",
      "epoch: 3, batch: 54, train_loss: 68.886284, train_error: None\n",
      "epoch: 3, batch: 64, train_loss: 117.126595, train_error: None\n",
      "epoch: 3, batch: 74, train_loss: 139.664169, train_error: None\n",
      "epoch: 3, batch: 84, train_loss: 89.663078, train_error: None\n",
      "epoch: 3, batch: 94, train_loss: 97.625748, train_error: None\n",
      "epoch: 3, batch: 104, train_loss: 102.347351, train_error: None\n",
      "epoch: 3, batch: 114, train_loss: 86.605179, train_error: None\n",
      "epoch: 3, batch: 124, train_loss: 73.655281, train_error: None\n",
      "epoch: 3, batch: 134, train_loss: 122.933807, train_error: None\n",
      "epoch: 3, batch: 144, train_loss: 117.291435, train_error: None\n",
      "epoch: 3, batch: 154, train_loss: 105.058723, train_error: None\n",
      "epoch: 3, batch: 164, train_loss: 88.296318, train_error: None\n",
      "epoch: 4, batch: 2, train_loss: 73.336250, train_error: None\n",
      "epoch: 4, batch: 12, train_loss: 63.014458, train_error: None\n",
      "epoch: 4, batch: 22, train_loss: 75.338791, train_error: None\n",
      "epoch: 4, batch: 32, train_loss: 75.079063, train_error: None\n",
      "epoch: 4, batch: 42, train_loss: 78.003342, train_error: None\n",
      "epoch: 4, batch: 52, train_loss: 72.267723, train_error: None\n",
      "epoch: 4, batch: 62, train_loss: 99.468704, train_error: None\n",
      "epoch: 4, batch: 72, train_loss: 71.745140, train_error: None\n",
      "epoch: 4, batch: 82, train_loss: 76.894615, train_error: None\n",
      "epoch: 4, batch: 92, train_loss: 126.484253, train_error: None\n",
      "epoch: 4, batch: 102, train_loss: 78.719887, train_error: None\n",
      "epoch: 4, batch: 112, train_loss: 69.143730, train_error: None\n",
      "epoch: 4, batch: 122, train_loss: 78.407944, train_error: None\n",
      "epoch: 4, batch: 132, train_loss: 77.821457, train_error: None\n",
      "epoch: 4, batch: 142, train_loss: 94.072273, train_error: None\n",
      "epoch: 4, batch: 152, train_loss: 73.842766, train_error: None\n",
      "epoch: 4, batch: 162, train_loss: 106.158012, train_error: None\n",
      "epoch: 5, batch: 0, train_loss: 97.287956, train_error: None\n",
      "epoch: 5, batch: 10, train_loss: 101.950897, train_error: None\n",
      "epoch: 5, batch: 20, train_loss: 118.855064, train_error: None\n",
      "epoch: 5, batch: 30, train_loss: 137.338745, train_error: None\n",
      "epoch: 5, batch: 40, train_loss: 100.592346, train_error: None\n",
      "epoch: 5, batch: 50, train_loss: 110.575470, train_error: None\n",
      "epoch: 5, batch: 60, train_loss: 66.624176, train_error: None\n",
      "epoch: 5, batch: 70, train_loss: 128.641586, train_error: None\n",
      "epoch: 5, batch: 80, train_loss: 83.576416, train_error: None\n",
      "epoch: 5, batch: 90, train_loss: 71.601273, train_error: None\n",
      "epoch: 5, batch: 100, train_loss: 81.226067, train_error: None\n",
      "epoch: 5, batch: 110, train_loss: 77.092484, train_error: None\n",
      "epoch: 5, batch: 120, train_loss: 67.378021, train_error: None\n",
      "epoch: 5, batch: 130, train_loss: 83.030273, train_error: None\n",
      "epoch: 5, batch: 140, train_loss: 79.823792, train_error: None\n",
      "epoch: 5, batch: 150, train_loss: 84.997849, train_error: None\n",
      "epoch: 5, batch: 160, train_loss: 93.893257, train_error: None\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(newsroom_dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optimizer = torch.optim.Adam(generator_model.parameters(),\n",
    "                             lr=LEARNING_RATE)\n",
    "# optimizer = torch.optim.Adagrad((generator_model.cuda() if USE_CUDA else generator_model).parameters(),\n",
    "#                                 lr=LEARNING_RATE, initial_accumulator_value=INITIAL_ACCUMULATOR_VALUE)\n",
    "model_manip = ModelManipulator(generator_model, optimizer, loss, error, use_cuda=USE_CUDA)\n",
    "train_stats, val_stats = model_manip.train(dataloader, NUM_EPOCHS, dataset_val=newsroom_dataset_dev, stats_every=10, verbose_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator_model, 'data/generator_test.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(training_values=train_stats, validation_values=val_stats, figure_name='summarization_training_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = newsroom_dataset_dev[0:5]\n",
    "generated_output = generator_model(batch['text'].cuda(), batch['text_length'].cuda())\n",
    "# generated_output = generator_model(batch['text'], batch['text_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,indices in enumerate(generated_output[1]):\n",
    "    text, l = batch['text'][i], batch['text_length'][i]\n",
    "    print(\"text\", get_index_words(text[:l], newsroom_dataset_train.words))\n",
    "    text, l = batch['summary'][i], batch['summary_length'][i]\n",
    "    print(\"summary\", get_index_words(text[:l], newsroom_dataset_train.words))\n",
    "    print(\"generated summary\", get_index_words(indices[:generated_output[2][i]], newsroom_dataset_train.words))\n",
    "print(generated_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
